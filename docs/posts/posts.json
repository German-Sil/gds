[
  {
    "path": "posts/2022-03-12-principle-component-analysis/",
    "title": "Principle Component Analysis",
    "description": "A way of understanding variable relationships",
    "author": [
      {
        "name": "Germán Silva",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\nOverview\nSummary:\nThis report was done as part of UCSB’s ESM 244 Advanced Data Analysis and looks at miscellaneous environmental variables round the world at the country level from Zander Venter on Kaggle. I explore the relationships between some of these variables by using principle component analysis (PCA). Variables were selected the minimize the number of rows removed by NA deletion. Final selected variables include accessibility to cities, cropland cover, tree canopy cover, annual mean rainfall and temperature, and cloudiness. The relationships between these variables are interpreted from a biplot and screeplot created from the PCA results.\nData Source\nData compiled by Zander Venter (2018). “Environmental variables for world countries”. Accessed from: https://www.kaggle.com/zanderventer/environmental-variables-for-world-countries\nPrinciple Component Analysis\nSet up code:\n\n\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\n\n## attach libraries\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggfortify)\n\n\n\nRead in the data:\n\n\n# Read in the data\nworld_env <- read_csv(here(\"data\", \"world_env_vars.csv\")) %>% # open csv\n  select(Country,\n         accessibility_to_cities, \n         cropland_cover, \n         tree_canopy_cover, \n         rain_mean_annual, \n         temp_mean_annual, \n         cloudiness) %>% # select the variables we want to explore\n  drop_na() # drops columns with NAs as PCA only works with numeric values\n\n\n\nPerform the PCA:\nPCA is easily performed in R with the prcomp() function. Scaling is performed to avoid any potential issues with comparisons between variables with different units. As PCA can only be done on numeric variables, the country names were removed for this step.\n\n\n# Performing the PCA\nworld_env_pca <- world_env %>% # pipe in the data\n  select(!Country) %>% # remove the country\n  prcomp(scale. = TRUE) # rescaling of data is performed\n\n\n\nCreate biplot and screeplot:\nA biplot is one way to interpret the results from a PCA. The arrows indicate the strength and correlation of the relationships of the variables to the principle components (PCs). The direction of the arrows relative to each other give a sense of their correlation or lack thereof in the axes determined by the different PCs.\n\n\n#Creating the biplot via autoplot()\n\nautoplot(world_env_pca, # pca data\n         data = world_env, # original data\n         loadings = TRUE, # show loadings\n         loadings.label = TRUE, # label loadings\n         loadings.colour = \"black\", # loading color\n         loadings.label.colour = \"black\", # loading label color\n         loadings.label.vjust = -.5) + # vertical justification\n  ggtitle(\"Principle Component 1 and 2 Biplot (SILVA)\") + # title\n  labs(x = \"Principle Component 1 (43.47%)\", \n       y= \"Principle Component 2 (23.42%)\")+ # axis labels \n   theme(plot.title = element_text(color = \"#5b4f41\", size = 16),\n            plot.background = element_rect(\"white\"),\n            panel.background = element_rect(\"#faf7f2\"),\n            panel.grid = element_line(linetype= \"longdash\",\n                                      color = \"#f0ece1\"),\n            axis.text = element_text(color = \"#5b4f41\"),\n            axis.title = element_text(color = \"#5b4f41\"),\n            strip.background = element_rect(\"white\"),\n            axis.line = element_line(color = \"#5b4f41\")) # change themes\n\n\n\n\nFigure 1: Biplot showing relationships between variables in PC1 and PC2. A few relationships between the variables stick out: 1) cloudiness, tree canopy cover, and mean annual rainfall seem to be positively correlated, 2) mean annual temperature and cropland are strongly negatively correlated with the angle between them being near 180o, 3) cropland cover and mean annual temp seem to have no to a weak correlation with cloudiness, tree canopy cover, and mean annual rainfall.\n\n\n\nScreeplots allow us to see how much of the variance in the data is explained by each of the PCs. Typically when ~80% of the variance is explained by the PCs is good.\n\n\n# Creating screeplot\nsd_vec <- world_env_pca$sdev # standard deviation\nvar_vec <- sd_vec^2 # variance\n\npc_names <- colnames(world_env_pca$rotation) # names of PCs\n\npct_expl_df <- data.frame(v = var_vec, # new data frame for screeplot; variance\n                          pct_v = var_vec / sum(var_vec), # percent of variance\n                          pc = fct_inorder(pc_names)) %>%  # orders rows\n  mutate(pct_label = paste0(round(pct_v * 100, 1), \"%\")) # adds character %\n\nggplot(pct_expl_df, aes(x = pc, y = v))+ # graphs via PC and variance\n  geom_col()+\n  geom_text(aes(label = pct_label), vjust = 0, nudge_y = 0.008)+\n  labs(x = \"Principle Components\",\n       y = \"Variances\") +\n  ggtitle(\"PCA Screeplot\")+\n  theme(plot.title = element_text(hjust = .5))\n\n\n\n\nFigure 2: Screeplot showing the amount of variance explained by each principle component.\n\n\n\nTakeaways:\nCloudiness, tree canopy cover, and mean annual rainfall have a positively correlated relationship with each other.\nThis makes conceptual sense as countries with more rain would have more cloudy days and likely be able to support more trees.\n\nMean annual temperature and cropland have a strong negatively correlated relationship with the angle between them being near 180o.\nWe might expect hotter countries (like Saudi Arabia) to have less arable land due to the extreme heat due to other environmental factors.\n\nCropland cover and mean annual temp seem to be not to weakly correlated with cloudiness, tree canopy cover, and mean annual rainfall.\nThe first two principle components account for 66.9% of the total variance in the data with over 80% being explained by the first three principle components.\n\n\n\n",
    "preview": "posts/2022-03-12-principle-component-analysis/principle-component-analysis_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2024-10-28T13:34:21-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-12-salmon-time-series-analysis/",
    "title": "Salmon Time Series Analysis",
    "description": "Some examples of time series visualization and wrangling",
    "author": [
      {
        "name": "Germán Silva",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\n\n\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\n\n# attach packages\nlibrary(tidyverse)\nlibrary(here)\nlibrary(lubridate)\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(slider)\nlibrary(patchwork)\n\n\n\nOverview\nSummary:\nThis report was done as part of UCSB’s ESM 244 Advanced Data Analysis class and explores a subset of the adult fish passage data from the Willamette Falls fish ladder on the Willamette River (Oregon, USA) from 2001-01-01 to 2010-12-31. The analyses focus on three types of salmon: coho, jack coho, and steelhead. A variety of time series visualization and analysis techniques are highlighted. Analysis of the data includes 1) visualization of the original times series, 2) seasonplots for the data, and 3) annual counts by species.\nCaption: Salmon swimming up a fish ladder. Photo by John Trax/Alamy Stock PhotoData Source:\nColumbia River Research, “Data Access in Real Time (DART) Adult Passage Graphics and Text”, http://www.cbr.washington.edu/dart/query/adult_graph_text\n\n\n# opening the data and subsetting for salmon of interest and creating time series data\nadult_pass <- read_csv(here('data', 'willamette_fish_passage.csv')) %>% \n  janitor::clean_names() %>%  \n  select(date, coho, steelhead, jack_coho) %>% \n  mutate(date = mdy(date)) %>% \n  as_tsibble(key = NULL, index = date)\n\n# changing NA values to 0\nadult_pass[is.na(adult_pass)] = 0\n\n# reformating data frame into a long format for easy of use in plots\nadult_pass_long <- adult_pass %>% \n  pivot_longer(!date, names_to = \"species\", values_to = \"count\")\n\n\n\nOriginal Time Series\nTime series data can be visualized and interpreted in many different ways. The code below shows how you would create a simple plot of the original time series data and how you might compare the the data between multiple groups (in this case fish species).\n\n\n# create the three ggplots visualizing the three time series\n\nggplot(adult_pass, aes(x = date))+\n  geom_line(aes(y = coho, color = \"Coho\")) +\n  geom_line(aes(y = steelhead, color = \"Steelhead\")) +\n  geom_line(aes(y = jack_coho, color = \"Jack Coho\")) +\n  labs(x = \"Date\",\n       y = \"# of Adult Fish\",\n       color = \"Legend\")+\n  scale_color_manual(values = c(\"Coho\" = \"#ff8362\", \n                                \"Steelhead\" = \"#DEADA1\", \n                                \"Jack Coho\" = \"#6d748c\"))+\n  ggtitle(\"Adult Fish Counts by Species at Willamette Falls\")+\n  theme(plot.title = element_text(color = \"#5b4f41\", hjust = 0.5),\n            plot.background = element_rect(\"white\"),\n            panel.background = element_rect(\"#faf7f2\"),\n            panel.grid = element_line(linetype= \"longdash\", \n                                      color = \"#f0ece1\"),\n            axis.text = element_text(color = \"#5b4f41\"),\n            axis.title = element_text(color = \"#5b4f41\"),\n            strip.background = element_rect(\"white\"),\n            axis.line = element_line(color = \"#5b4f41\"))\n\n\n\n\nFigure 1: Plots showing the counts of the fish for each date. From each plot we can get a general sense of how the three fish differ in their seasonality.\n\n\n\nTakeaway Points:\nSteelhead numbers seem to spike at a different time of year than Coho and Jack Coho\nCoho and Jack Coho numbers are much more punctuated than the Steelhead numbers\nCoho numbers seem to be increasing over time\nSeasonplots\nSeasonplots are another way that you can interpret and analyze time series data. Seasonplots allow you to compare seasons (in this case years) to one another so you can get a sense as to how the data is changing from season to season and year to year. Here we use gg_season() and {patchwork} to make a comprehensive plot that shows how seasons change from year to year for each fish.\n\n\n# create a season plot for each species\ncoho_seaon <- adult_pass %>% \n  gg_season(y = coho) +\n  theme(plot.title = element_text(color = \"#5b4f41\", hjust = 0.5),\n            plot.background = element_rect(\"white\"),\n            panel.background = element_rect(\"#faf7f2\"),\n            panel.grid = element_line(linetype= \"longdash\", color = \"#f0ece1\"),\n            axis.text = element_text(color = \"#5b4f41\"),\n            axis.title = element_text(color = \"#5b4f41\"),\n            strip.background = element_rect(\"white\"),\n            axis.line = element_line(color = \"#5b4f41\"),\n            legend.position = \"none\")+\n  scale_colour_viridis_c(direction = -1) +\n  labs(x = \"Date\",\n       y = \"\")+\n  ggtitle(\"Coho\")\n\nsteelhead_season <- adult_pass %>% \n   gg_season(y = steelhead) +\n  theme(plot.title = element_text(color = \"#5b4f41\", hjust = 0.5),\n            plot.background = element_rect(\"white\"),\n            panel.background = element_rect(\"#faf7f2\"),\n            panel.grid = element_line(linetype= \"longdash\", color = \"#f0ece1\"),\n            axis.text = element_text(color = \"#5b4f41\"),\n            axis.title = element_text(color = \"#5b4f41\"),\n            strip.background = element_rect(\"white\"),\n            axis.line = element_line(color = \"#5b4f41\"),\n            legend.position = \"none\")+\n  scale_color_viridis_c(direction = -1)+\n  labs(x = \"Date\",\n       y =  \"\")+\n  ggtitle(\"Steelhead\")\n\njack_season <- adult_pass %>% \n   gg_season(y = jack_coho) +\n  theme(plot.title = element_text(color = \"#5b4f41\", hjust = 0.5),\n            plot.background = element_rect(\"white\"),\n            panel.background = element_rect(\"#faf7f2\"),\n            panel.grid = element_line(linetype= \"longdash\", color = \"#f0ece1\"),\n            axis.text = element_text(color = \"#5b4f41\"),\n            axis.title = element_text(color = \"#5b4f41\"),\n            strip.background = element_rect(\"white\"),\n            axis.line = element_line(color = \"#5b4f41\"))+\n  scale_color_viridis_c(direction = -1)+\n  labs(x = \"Date\",\n       y = \"# of Adult Fish\",\n       color = \"# of Season\")+\n  ggtitle(\"Jack Coho\")\n\npatchwork <- coho_seaon /jack_season / steelhead_season\n\npatchwork\n\n\n\n\nFigure 2: Season Plots for all ten years for each species of fish. Earlier seasons are colored in a light color and darker colors represent later seasons. Both types of Coho peak around October, while Steelhead peak around June.\n\n\n\nTakeaway Points:\nCoho and Jack Coho salmon have similar seasonality\nCoho and Jack Coho numbers seem to be on the rise as time goes on\nSteelhead numbers peak around April to July and don’t show as clear of an increase as Jack Coho and Coho\nAnnual counts by species\nLastly, time series can be compiled and wrangled to get different formats of datasets to visualize a variety of things (like annual counts). The tsibble data frame format makes this easy and allows us to index by year and group by species easily making it easy to compile a daily data set into a yearly data set that we can use to make comparisons of annual counts.\n\n\nadult_annual <- adult_pass_long %>% \n  index_by(year = ~year(.)) %>% \n  group_by(species, year) %>% \n  summarize(annual_sum = sum(count))\n\nspecies.labs <- c(\"coho\" = \"Coho\", \n                  \"jack_coho\" = \"Jack Coho\", \n                  \"steelhead\" = \"Steelhead\")\n\nggplot(adult_annual, aes(x = year, y = annual_sum, fill = species))+\n  geom_bar(stat = \"identity\")+\n  facet_wrap(~species, labeller = labeller(species = species.labs))+\n  scale_fill_manual(values = c(\"coho\" = \"#ff8362\", \n                               \"steelhead\" = \"#DEADA1\", \n                               \"jack_coho\" = \"#6d748c\"))+\n  ggtitle(\"Annual Adult Fish Counts by Species\")+\n  labs(x = \"Year\",\n       y = \"Annual Counts\",\n       fill = \"Species\")+\n  theme(plot.title = element_text(color = \"#5b4f41\", hjust = 0.5),\n            plot.background = element_rect(\"white\"),\n            panel.background = element_rect(\"#faf7f2\"),\n            panel.grid = element_line(linetype= \"longdash\", \n                                      color = \"#f0ece1\"),\n            axis.text = element_text(color = \"#5b4f41\"),\n            axis.title = element_text(color = \"#5b4f41\"),\n            strip.background = element_rect(\"white\"),\n        strip.text = element_text(color = \"#5b4f41\"),\n            axis.line = element_line(color = \"#5b4f41\"),\n        legend.position = \"none\")\n\n\n\n\nFigure 3: Bar plots showing total annual counts of adult fish by sepcies from 2000 to 2010. Steelhead have the largest number of individuals, which could be due to having a longer season than the other two species.\n\n\n\nTakeaway Points:\nJack Coho have the lowest annual counts out of the three species assessed\nAnnual Coho numbers seem to be on the rise between 2000 and 2010.\nDespite Coho having the largest peak on daily passages, as seen in Fig. 1, steelhead have the largest number of annual passings.\n\n\n\n",
    "preview": "posts/2022-03-12-salmon-time-series-analysis/salmon-time-series-analysis_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2024-10-28T13:34:21-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-12-spatial-data-visualization-and-point-pattern-analysis/",
    "title": "Spatial Data Visualization and Point Pattern Analysis",
    "description": "Spatial data and knowing how to visualize and analyze it is an important skill in geography",
    "author": [
      {
        "name": "Germán Silva",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [],
    "contents": "\nOverview\nSummary:\nThis entry highlight spatial data visualization done and point pattern analysis done as part of UCSB’s ESM 244. This code provides two examples of spatial data visualization for oil spill data in California and point pattern analysis to assess whether the spills are clustered or not. The exploratory data visualization is in an interactive format, while the choropleth map is a static map in a finalized format. Both highlight how the same data can be shown in different ways to visualize different aspects of the spatial data. For example, the full data set is likely useful for interactive exploration, but would make for a messy map if we were trying to convey counts in an easy to understand format. A choropleth makes the data easy to interpret, but obscures where exactly the events occurred.\nData Citation: CA Department of Fish and Wildlife, Office of Spill Prevention and Response, “Oil spill Incident Tracking [ds394],” https://gis.data.ca.gov/datasets/7464e3d6f4924b50ad06e5a553d71086_0/explore?showTable=true\n\n\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, results = FALSE)\n\n# attach libraries\nlibrary(spatstat)\nlibrary(tmap)\nlibrary(maptools)\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(here)\n\n\n\nExploratory Map\nRead in the data\nWe read in the data for the oil spills and CA counties as simple features (or “sf”) using read_sf().\n\n\n# spill data\nspills <- read_sf(here('data', 'Oil_Spill_Incident_Tracking_[ds394]',\n                       \"Oil_Spill_Incident_Tracking_[ds394].shp\")) %>% \n  janitor::clean_names()\n\n# county shapefile\nca_counties <- read_sf(here('data', 'ca_counties', \n                            'CA_Counties_TIGER2016.shp')) %>% \n  janitor::clean_names()\n\n\n\nCreate interactive exploratory map\nWe need to first explore the data to get a sense of where things lie in geographic space and how we may want to analyse the data. Here we accomplish this through an interactive map.\n\n\n# interactive mode\ntmap_mode('view')\n\n# creation of exploratory map with both layers\ntm_shape(ca_counties)+\n  tm_fill(col = 'black', alpha = 0.3)+\n  tm_borders(col = 'black', lwd = 0.5)+\n  tm_shape(spills) +\n  tm_dots()\n\n\n\nCreation of visualization for 2008 Inland Spills by County\nSubset and join data\nsf objects can be used much like a data frame in R and this allows us to use typical data wrangling technqiues on them. Here we subset the oil spill point layer for events that occurred inland in 2007 and spatially join it with the CA counties layer.\n\n\n# subset of data\nspills_inland <- spills %>% \n  filter(inlandmari == \"Inland\") %>% \n  mutate(dateofinci = lubridate::ymd(dateofinci),\n         year = lubridate::year(dateofinci)) %>% \n  filter(year == 2008)\n\n# spatial joining\nca_spills <- ca_counties %>% \n  st_join(spills_inland)\n\n\n\nObtain the number of 2008 spills by county\nOnce the two layers are joined, we can use summarize() to obtain a count of the number of oil spills by county.\n\n\nca_spill_count <- ca_spills %>% \n  group_by(name) %>% \n  summarize(n_records = sum(!is.na(oesnumber)))\n\n\n\nCreate ggplot for visualization\nOnce we have the number of oil spills by county, we can create a choropleth that will visualize that data in an easy to interpret static map.\n\n\nggplot(ca_spill_count, aes(fill = n_records))+\n  geom_sf() +\n  scale_fill_viridis_c(option = \"A\",direction = -1)+\n  theme(plot.title = element_text(color = \"#5b4f41\", size = 16),\n            plot.background = element_rect(\"white\"),\n            panel.background = element_rect(\"#faf7f2\"),\n            panel.grid = element_line(linetype= \"longdash\", color = \"#f0ece1\"),\n            axis.text = element_text(color = \"#5b4f41\"),\n            axis.title = element_text(color = \"#5b4f41\"),\n            strip.background = element_rect(\"white\"),\n            axis.line = element_line(color = \"#5b4f41\"))+\n  labs(x = \"Latitude\",\n       y = \"Longitude\",\n       fill = \"# of Spills\")+\n  ggtitle(\"2008 California Oil Spills by County\")\n\n\n\n\nFigure 1: Map of California Counties colored according to the number of oil spills that occured in 2008. Lighter colors indicate fewer icidents than darker colors.\n\n\n\nPoint Pattern Analysis (G-function)\nWe can also perform point pattern analysis on the data to see understand more about the spatial distribution of the data. This can be done using a G-function to assess nearest neighbors between points.\n\n\nspills_sp <- as(spills, \"Spatial\")\nspills_ppp <- as(spills_sp, \"ppp\")\n\nca_sp <- as(ca_counties, \"Spatial\")\nca_win <- as(ca_sp, \"owin\")\n\nspills_full <- ppp(spills_ppp$x, spills_ppp$y, window = ca_win)\n\nr_vec <- seq(0, 25000, by = 100) # make a sequence of distances for G(r)\n\n\ngfunction <- envelope(spills_full, fun = Gest, r = r_vec, nsim = 10, nrank = 2)\n\ngfunction_long <- gfunction %>% \n  as.data.frame() %>% \n  pivot_longer(cols = obs:hi, names_to = \"model\", values_to = \"g_val\")\n\nggplot(gfunction_long, aes(x = r, y = g_val, group = model))+\n  geom_line(aes(color = model)) +\n  scale_color_manual(values = calecopal::cal_palette(name = \"sierra1\",\n                                                     n = 4, \n                                                     type = \"discrete\"))+\n  theme(plot.title = element_text(color = \"#5b4f41\", size = 16),\n            plot.background = element_rect(\"white\"),\n            panel.background = element_rect(\"#faf7f2\"),\n            panel.grid = element_line(linetype= \"longdash\", color = \"#f0ece1\"),\n            axis.text = element_text(color = \"#5b4f41\"),\n            axis.title = element_text(color = \"#5b4f41\"),\n            strip.background = element_rect(\"white\"),\n            axis.line = element_line(color = \"#5b4f41\"))+\n  labs(x = \"Distance\",\n       y = \"% of Point Pairs\",\n       color = \"Model\")+\n  ggtitle(\"Nearest Neighbor Point Pattern Analysis by G-Function\")\n\n\n\n\nFigure 2: A G-function plot used assess whether events are spatially random or not. As the G(r) values (% of Points pairs with a neighbor within the distance) are higher for the observed values than the model, we can conclude that CA oil spills are a clustered phenomena.\n\n\n\n\n\n\n",
    "preview": "posts/2022-03-12-spatial-data-visualization-and-point-pattern-analysis/spatial-data-visualization-and-point-pattern-analysis_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2024-10-28T13:34:21-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-20-welcome/",
    "title": "Data Visualization and T-tests",
    "description": "Exploring data and performing a type of hypothesis testing",
    "author": [
      {
        "name": "Germán Silva",
        "url": {}
      }
    ],
    "date": "2022-02-20",
    "categories": [],
    "contents": "\nOverview\nThis blog post highlights an example of code done for an assignment in ESM 206 at UCSB’s Bren School of Environmental Science and Managment. The task looks at data from the 2017 update by Lightwood, D. of the Jornada LTER lizard pitfall data from 1989 to 2006. The main question examined in the code is the differences in length (mm) between females of two lizard species: Cnemidophorus uniparens (Desert Grassland lizard) and Cnemidophorus tessalatus (Colorado Checkered lizard). Methods used to address this question include: exploratory data visualization, box and scatter plots, summary statistics, t-tests, and Cohen’s d, among other general data science practices. Each code chunk will be proceeded with explanations for the code.\nCode and Explanations\nThis is the set up chunk which allows for the setting of rendering options, such as the suppression of warnings and messages, but also for the attaching of packages that will be useful through out the task at hand.\n\n\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\n# attach packages\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggbeeswarm)\n\n\n\nBy using read_csv() we are able to open a .csv file into the code. The subsequent code allows us to clean and wrangle the data into a more manageable format and removes excess information that is not related to the task we are performing. In this case, I kept columns related to sex, species, and body length, and then filtered for rows that contained the individuals of the species and sex that were were comparing.\n\n\n# Creation of a stored object that includes a simplified version of the data\nlizards <- read_csv(here(\"_posts\", \"2022-02-20-welcome\",\"data\", \"lizards.csv\")) %>% # read in the data\n  select(spp, sex, total_length) %>% # \n  filter(sex == \"F\",\n         spp %in% c(\"CNUN\", \"CNTE\"))\n\n\n\nLooking at the Data\nData visualization is an important part of exploring your data so that you can see what the data looks like before making a decision on how to analyze it. This set of code makes a combined point and box plot as a way of showing data spread and data summary statistics. A similar plot can be achieved by making violin plots, but I find this way more visually appealing.\n\n\nggthemr::ggthemr('dust')\n\nggplot(lizards, aes(x=spp, y=total_length))+ # call data and mapping\n  geom_beeswarm(aes(fill=spp), shape= 21, color = \"brown\") + # add beeswarm plot\n  geom_boxplot(aes(fill=spp),\n               alpha=0.45,\n               width = 0.2,\n               outlier.color = NA) + # overlay boxplot\n  scale_fill_manual(values = calecopal::cal_palette(\"coastaldune1\",\n                                                     n= 2, \n                                                     type = \"discrete\"))+\n  scale_x_discrete(labels = c(\"Cnemidophorus tessalatus\", \n                              \"Cnemidophorus uniparens\")) + # x-axis tick labels\n  ggtitle(\"Total Length of Observed Female Lizards by Species\") + # title\n  theme(legend.position = \"none\") + # theme edits\n  labs(x= \"Species Name\",\n       y= \"Total Length (mm)\") # add labels for axes\n\n\n\n\nFigure 1: Boxplots and respective data point showing the total length for female lizards by respective species. Plots indicate that median female Cnemidophorus uniparens total length is less than the median female Cnemidophorus tessalatus length.\n\n\n\nOnce data has been visualized, it is important to obtain actual values for the summary statistics so you can talk in terms of actual numbers. This can often be achieved with the creation of a summary data frame and a table. The group_by() and summarise() functions are helpful in the creation of a summary data frame, with kable() being able to create a table from the resulting data frame.\n\n\n# Obtain summary stats\nlizards_summary <- lizards %>% \n  mutate(spp = case_when(\n    spp == \"CNTE\" ~ \"C. tessalatus\",\n    spp == \"CNUN\" ~ \"C. uniparens\"\n  )) %>% \n  group_by(spp) %>% \n  summarise(mean = mean(total_length, na.rm = TRUE),\n            sd = sd(total_length, na.rm = TRUE),\n            sample_size = n())\n# Convert into finalized table\nkableExtra::kable(lizards_summary,\n                  col.names = c(\"Species\", \n                                \"Mean Length (mm)\", \n                                \"Standard Deviation (mm)\", \n                                \"Sample Size\"),\n                  caption = \"Table 1 Summary Statistics for Female Lizards by Species\") %>% \n  kableExtra::kable_classic(full_width = FALSE)\n\n\n\nTable 1: Table 1 Summary Statistics for Female Lizards by Species\n\n\nSpecies\n\n\nMean Length (mm)\n\n\nStandard Deviation (mm)\n\n\nSample Size\n\n\nC. tessalatus\n\n\n244.8929\n\n\n47.32499\n\n\n28\n\n\nC. uniparens\n\n\n147.6000\n\n\n34.55341\n\n\n47\n\n\nHypothesis Testing\nQuestion:\nIs there a significant difference in total length difference in mean total length between female lizards for the two species?\nTo answer this question, with a parametric hypothesis test, we must first check for any violation of the assumptions for a t-test. This can be achieved with a histogram and a QQplot. The code below creates two simple versions of both plots to check for these violations.\n\n\n# Explore data to ensure that hypothesis testing can be done without violation of assumptions\nggplot(data = lizards, aes(x= total_length))+\n  geom_histogram()+ \n  facet_wrap(~spp)\n\n\n\nggplot(lizards, aes(sample=total_length))+\n  geom_qq()+\n  facet_wrap(~spp)\n\n\n\n\nBoth diagnostics are a bit concerning. Histograms seems to be somewhat close to normal, but QQ plots are a bit more concerning for assumptions of hypothesis testing. Law of large numbers would imply that means of samples would be normally distributed, both have moderately sized samples (n= 29 and 47 respectively). With Law of Large numbers, we can proceed with the t-test.\nTo perform a t-test, we need our data to be free of NA values. We first filter the data for the individuals from each respective group and remove the NA values. Two metrics are used to get an understanding of the differences between the two populations of lizards: Cohen’s d and t-test p-value. To get these metrics, I used effsize::cohen_d() and t.test().\n\n\n# Performing a t-test\ncnte <- lizards %>% \n  filter(spp == \"CNTE\") %>% \n  na.omit()\ncnun <- lizards %>% \n  filter(spp == \"CNUN\") %>% \n  na.omit()\ncohen_d <- effsize::cohen.d(cnte$total_length, cnun$total_length)\nt_test <- t.test(cnte$total_length, cnun$total_length)\n\n\n\nAnswer:\nWhile there may be potential violations of assumptions, hypothesis testing can be done under the assumption that the law of large numbers applies to this data sample. Hypothesis testing shows that the mean length difference of 97.29 mm is statistically significant (p-value = 3.75 * 10-12), with the p-value indicating that there is a 3.75 * 10-10 % chance that we would get sample means that were at least this different if the true means are the same, and a Cohen’s d (d = 2.42) indicating that there is a true and noticeable difference between the mean lengths of the two species.\nReferences:\nBui, An, Lowman, Heili, Guerra, Ana Sofia, and Miller-ter Kuile, Ana (2021). “calecopal: A California-inspired Package of Color Palettes.” R package version 0.1.0.\nClarke, Erik and Sherrill-Mix, Scott (2017). ggbeeswarm: Categorical Scatter (Violin Point) Plots. R package version 0.6.0. https://CRAN.R-project.org/package=ggbeeswarm\nLightfoot, D. (2017). Lizard pitfall trap data (LTER-II, LTER-III) ver 36. Environmental Data Initiative. https://doi.org/10.6073/pasta/ff37b4cace16a9943575f3fd7067064e\nMüller, Kirill (2020). “here: A Simpler Way to Find Your Files.” R package version 1.0.1. https://CRAN.R-project.org/package=here\nTorchiano M (2020). effsize: Efficient Effect Size Computation. doi: 10.5281/zenodo.1480624 (URL: https://doi.org/10.5281/zenodo.1480624), R package version 0.8.1, <URL: https://CRAN.R-project.org/package=effsize>.\nWickham et al., (2019). “Welcome to the tidyverse.” Journal of Open Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686\nZhu, Hao (2021). “kableExtra: Construct Complex Table with ‘kable’ and Pipe Syntax.” R package version 1.3.4. https://CRAN.R-project.org/package=kableExtra\n\n\n\n",
    "preview": "posts/2022-02-20-welcome/welcome_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2024-10-28T13:34:21-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-18-welcome/",
    "title": "Let's Talk Reproducible Data Science",
    "description": "Learn about two tools to help you work with data in a more reproducible way",
    "author": [
      {
        "name": "Germán Silva",
        "url": {}
      }
    ],
    "date": "2022-02-02",
    "categories": [],
    "contents": "\nIntro\nI’m sure that if you are least somewhat familiar with coding then you have had to deal with the issue of having to navigate file paths and working directories. Maybe you have even had to work with someone’s code and had to update broken file paths or reset hard coded working directories? If that sounds like you, you might have thought to yourself, “F**k this! There must be a much easier way to deal with this stuff!”\nWell there is no need to swear anymore as there totally is a much easier way to do this and in a way that will be reproducible for others to do! Let me introduce you to two little things that will perhaps make your life much easier: the {here} package and Rprojects.\nSo what are {here} and Rprojects?\nSimply put they are a way to work within self-contained folders with relative file paths.\nYou see a R project is like making a partitioned folder that acts itself a bit like a working directory, with all subdirectories within the folder containing the .Rproj file also being connected to that directory. So if someone needs to work with your code and data, you would have a nice self contained folder that you can zip up and send to them and they would be able to work with it right away without having to reset working directories or figure out where all the data is all over again.\nHowever, what happens when you have a different operating system that your colleague? MACos and Windows use different notations for their filepaths and then what good would having a Rproject do for you?\nWell this is where using relative filepaths helps you out and {here} is well… here to help! You see {here} functions by assuming the place that your Rproject is at is the home directory and then merely looks for directories by name rather than by absolute paths. So instead of having to say read_csv(\"C:\\Users\\German\\Documents\\UCSB\\data\\example.csv\"), you would instead write read_csv(here(\"data\", \"example.csv\")), not only is it much less to write, but it ignores differences in filepath notation and allows for you to much more easily reproduce code and share it with others without having to worry about the differences in operating systems or filepath structures!\nI hope that this has been insightful and that you play with some more reproducible data science!\n\n\n\n",
    "preview": {},
    "last_modified": "2024-10-28T13:34:21-07:00",
    "input_file": {}
  }
]
